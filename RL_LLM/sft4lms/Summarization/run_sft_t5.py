import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


import argparse
import numpy as np
from sklearn.metrics import *
import nltk
import re

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from transformers import EarlyStoppingCallback
from datasets import load_dataset, load_metric, Dataset
from bert_score import BERTScorer
import evaluate

from sft4lms.Summarization.data_loader import get_data_split, get_general_data_split, get_attack_data_split, EXTRACTION_PREFIX, SUMMARIZATION_PREFIX, SPLIT
from rl4lms.envs.text_generation.gpt3_utils import GPT3, avoid_keywords

from transformers import (
    CONFIG_MAPPING,
    MODEL_MAPPING,
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    SchedulerType,
    get_scheduler,
    set_seed,
)

def load_model(output_dir, model_path, strategy):
    if output_dir and os.path.exists(output_dir):
        if "checkpoint" in output_dir:
            model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)
            tokenizer = AutoTokenizer.from_pretrained(output_dir)
        else:
            if strategy == 'load_last':
                latest_checkpoint_idx = 0
                dir_list = os.listdir(output_dir) # find the latest checkpoint
                for d in dir_list:
                    if "checkpoint" in d and "best" not in d:
                        checkpoint_idx = int(d.split("-")[-1])
                        if checkpoint_idx > latest_checkpoint_idx:
                            latest_checkpoint_idx = checkpoint_idx
                if latest_checkpoint_idx > 0 and os.path.exists(os.path.join(output_dir, f"checkpoint-{latest_checkpoint_idx}")):
                    ft_model_path = os.path.join(output_dir, f"checkpoint-{latest_checkpoint_idx}")
                    model = AutoModelForSeq2SeqLM.from_pretrained(ft_model_path)
                    tokenizer = AutoTokenizer.from_pretrained(ft_model_path)
                    return model, tokenizer
            elif strategy == 'load_best':
                ft_model_path = os.path.join(output_dir, f"best_checkpoint")
                if os.path.exists(ft_model_path):
                    model = AutoModelForSeq2SeqLM.from_pretrained(ft_model_path)
                    tokenizer = AutoTokenizer.from_pretrained(ft_model_path)
                    return model, tokenizer
            elif strategy == 'load_initial':
                model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                return model, tokenizer

    # load pretrained model for hf
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer


def fine_tune_hf(
    task,
    model_name,
    dataset_name,
    n_train,
    push_to_hub,
    model,
    tokenizer,
    extraction_source,
    max_ctx_len,
    max_tgt_len,
    length_penalty,
    no_repeat_ngram_size,
    output_dir,
    train_data,
    val_data,
    test_data,
    epochs,
    train_batch_size,
    eval_batch_size,
    logging_steps,
    save_total_limit,
    early_stopping_patience,
    learning_rate,
    seed,
    do_train,
    do_inference
):  
    
    def preprocess_function_for_summarization(batch):
        inputs = [SUMMARIZATION_PREFIX + doc for doc in batch["article"]]
        targets = [doc for doc in batch["summary"]]
        model_inputs = tokenizer(inputs, max_length=max_ctx_len, truncation=True)
        labels = tokenizer(targets, max_length=max_tgt_len, truncation=True)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    def preprocess_function_for_extraction(batch):
        #å°†batchä¸­çš„ "article" åˆ—ä¸­çš„æ¯ä¸ªæ–‡æ¡£å‰é¢åŠ ä¸Š EXTRACTION_PREFIX = "Extract the keywords: "ã€‚
        inputs = [EXTRACTION_PREFIX + doc for doc in batch["article"]]
        targets = [doc for doc in batch["target"]]
        #è¾“å…¥æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ max_ctx_len
        model_inputs = tokenizer(inputs, max_length=max_ctx_len, truncation=True)
        #ç›®æ ‡æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œå¹¶ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ max_tgt_len
        labels = tokenizer(targets, max_length=max_tgt_len, truncation=True)
        model_inputs["labels"] = labels["input_ids"]
        #æ¨¡å‹è¾“å…¥åŒ…æ‹¬å¤„ç†åçš„è¾“å…¥æ–‡æ¡£å’Œç›®æ ‡
        # for input in inputs:
        #     print('len(input):',len(input))
        # for target in targets:
        #     print('len(target):',len(target))
        #     #print('target:', target)
        # for input in model_inputs['input_ids']:
        #     print('len(model_inputs):',len(input))
        # for target in model_inputs["labels"]:
        #     print('len(model_labels):',len(target))
            #print('model_labels:', target)
        #print("targetsâ€”â€”è®­ç»ƒ", targets)

        return model_inputs

    #å°†è¾“å…¥çš„æ ‡ç­¾åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨ç©ºæ ¼è¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå­—ç¬¦ä¸²é”®ã€‚è¿™æ ·ï¼ŒåŸå§‹çš„æ ‡ç­¾åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½è¢«ç©ºæ ¼åˆ†éš”å¹¶ç»„æˆäº†ä¸€ä¸ªå”¯ä¸€çš„é”®ã€‚
    def convert_label_to_key(labels):
        return " ".join([str(i) for i in labels])
    
    def dataset_summary_mapping(dataset):
        mapping_dict = {}
        for d in dataset:
            #è·å–å½“å‰æ•°æ®çš„æ ‡ç­¾ï¼Œä¹Ÿå°±æ˜¯æºæ•°æ®ä¸­çš„"target"ï¼Œæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²,æ¯”å¦‚ï¼šMr Paxman; Mr Miliband; Mr Marr; Labour leader Ed Miliband; Mr Cameron.
            label = d['labels']
            #è°ƒç”¨ convert_label_to_key å‡½æ•°ï¼Œå°†æ ‡ç­¾è½¬æ¢ä¸ºç‰¹å®šçš„é”®ï¼ˆlabel_keyï¼‰
            #å¦‚æœè¾“å…¥çš„æ ‡ç­¾åˆ—è¡¨æ˜¯ [1, 2, 3]ï¼Œåˆ™é€šè¿‡è¯¥å‡½æ•°è½¬æ¢åçš„é”®ä¸º "1 2 3"
            label_key = convert_label_to_key(label)
            #print('label_key', label_key)
            #å°†ç»è¿‡è½¬æ¢åçš„æ ‡ç­¾ä½œä¸ºé”®ï¼Œå°†æ•°æ®çš„æ‘˜è¦ï¼ˆsummaryï¼‰ä½œä¸ºå€¼ï¼Œæ·»åŠ åˆ°mapping_dictå­—å…¸ä¸­ã€‚
            mapping_dict[label_key] = d['summary']
            print(mapping_dict)
        return mapping_dict

    # training the model with Huggingface ğŸ¤— trainer
    #{"article": article, "summary": summary, "id": id, "phrases": selected_phrases, "target": target}
    train_dataset = Dataset.from_dict(train_data)
    val_dataset = Dataset.from_dict(val_data)
    test_dataset = Dataset.from_dict(test_data)
    # ä»è®­ç»ƒæ•°æ®é›†ä¸­ç§»é™¤åä¸º "phrases" çš„åˆ—ã€‚phrasesæ˜¯å…³é”®è¯åˆ—è¡¨ï¼Œè¿™ä¸ªåœ¨æ•°æ®ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
    train_dataset = train_dataset.map(remove_columns=["phrases"])
    val_dataset = val_dataset.map(remove_columns=["phrases"])
    test_dataset = test_dataset.map(remove_columns=["phrases"])
    # tokenize the dataset
    if task == 'summarization':
        train_dataset = train_dataset.map(preprocess_function_for_summarization, batched=True, remove_columns=["target", "article", "summary"])
        val_dataset = val_dataset.map(preprocess_function_for_summarization, batched=True, remove_columns=["target", "article"])
        test_dataset = test_dataset.map(preprocess_function_for_summarization, batched=True, remove_columns=["target", "article"])
    elif task == 'extraction':
        # åº”ç”¨é¢„å¤„ç†å‡½æ•°ä¹‹å‰ï¼Œå°†ä»æ•°æ®é›†ä¸­ç§»é™¤åremove_columnsåˆ—
        # æ¨¡å‹è¾“å…¥train_datasetåˆå¢åŠ äº†åˆ†è¯å™¨å¯¹å¤„ç†åçš„è¾“å…¥æ–‡æ¡£[â€œarticleâ€]å’Œç›®æ ‡[â€œtargetâ€]
        train_dataset = train_dataset.map(preprocess_function_for_extraction, batched=True, remove_columns=["target", "article", "summary"])
        val_dataset = val_dataset.map(preprocess_function_for_extraction, batched=True, remove_columns=["target", "article"])
        test_dataset = test_dataset.map(preprocess_function_for_extraction, batched=True, remove_columns=["target", "article"])

    # print('train_dataset:',train_dataset)
    # train_dataset: Dataset({
    #     features: ['id', 'input_ids', 'attention_mask', 'labels'],
    #     num_rows: 10
    # })
    # print('val_dataset:', val_dataset)
    # val_dataset: Dataset({
    #     features: ['summary', 'id', 'input_ids', 'attention_mask', 'labels'],
    #     num_rows: 5
    # })

    # mapping from label to summaries
    #æ•°æ®é›†ä¸­çš„æ ‡ç­¾æ˜ å°„åˆ°å¯¹åº”çš„æ‘˜è¦ï¼Œå³é€šè¿‡æ ‡ç­¾æŸ¥æ‰¾ç›¸åº”çš„æ‘˜è¦ä¿¡æ¯ã€‚
    val_summary_mapping = dataset_summary_mapping(val_dataset)
    test_summary_mapping = dataset_summary_mapping(test_dataset)
    #ä¸¤ä¸ªå­—å…¸ï¼ˆval_summary_mappingå’Œtest_summary_mappingï¼‰åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„å­—å…¸val_test_summary_mappingã€‚**ç¬¦å·ç”¨äºå°†å­—å…¸è§£åŒ…å¹¶åˆå¹¶åˆ°æ–°çš„å­—å…¸ä¸­ã€‚
    val_test_summary_mapping = {**val_summary_mapping, **test_summary_mapping}
    # remove unused columns summaryï¼Œå¾—åˆ°çš„ val_datasetã€test_dataset å°†æ˜¯æ²¡æœ‰ "summary" åˆ—çš„æ–°æ•°æ®é›†ã€‚
    val_dataset = val_dataset.map(remove_columns=["summary"])
    test_dataset = test_dataset.map(remove_columns=["summary"])


    #print('train_dataset', train_dataset)
    # train_dataset
    # Dataset({
    #     features: ['id', 'input_ids', 'attention_mask', 'labels'],
    #     num_rows: 10
    # })
    # print('val_dataset', val_dataset)
    # val_dataset
    # Dataset({
    #     features: ['id', 'input_ids', 'attention_mask', 'labels'],
    #     num_rows: 5
    # })
    # customized metrics
    #metric = load_metric("rouge")
    #rouge_metric = evaluate.load("./metric/rouge.py")
    rouge_metric = load_metric("./metric/rouge.py")

    all_predictions = []
    all_references = []
    def compute_rouge_metrics(eval_pred):

        # print('eval_pred [æ˜¯id]ï¼š', eval_pred)
        predictions, labels = eval_pred
        #skip_special_tokens=Trueï¼Œå°†åœ¨è§£ç è¿‡ç¨‹ä¸­è·³è¿‡ç‰¹æ®Šæ ‡è®°ï¼Œæ¯”å¦‚èµ·å§‹æ ‡è®°ï¼ˆä¾‹å¦‚ [CLS] æˆ– <s>ï¼‰å’Œç»ˆæ­¢æ ‡è®°ï¼ˆä¾‹å¦‚ [SEP] æˆ– </s>ï¼‰ã€‚
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        # Replace -100 in the labels as we can't decode them.
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        print('decoded_preds0ï¼š', decoded_preds)
        print('decoded_labels0ï¼š', decoded_labels)
        
        # Rouge expects a newline after each sentence
        # strip() æ–¹æ³•ç”¨äºç§»é™¤å­—ç¬¦ä¸²ä¸¤ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚
        # å°†é€šè¿‡ sent_tokenize åˆ†å‰²å¾—åˆ°çš„å¥å­åˆ—è¡¨é‡æ–°ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå¥å­ä¹‹é—´ç”¨æ¢è¡Œç¬¦ \n è¿æ¥ã€‚
        decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
        decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

        print('decoded_preds1ï¼š', decoded_preds)
        print('decoded_labels1ï¼š', decoded_labels)
        
        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
        # Extract a few results
        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}


        # Add mean generated length
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
        result["gen_len"] = np.mean(prediction_lens)

        #æ·»åŠ  bert_score
        all_predictions.extend(decoded_preds)
        all_references.extend(decoded_labels)
        
        return {k: round(v, 4) for k, v in result.items()}
    
    def compute_hit_metrics(eval_pred):

        # print('eval_pred [æ˜¯id]ï¼š',eval_pred)
        predictions, labels = eval_pred
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

        hint_precisions, hint_hit_key_nums, hint_key_nums = [], [], []
        hint_recalls, hint_hit_word_nums, hint_word_nums = [], [], []
        assert len(labels) == len(decoded_preds)
        for i  in range(len(decoded_preds)):
            #decoded_preds: è¿™æ˜¯ä¸€ä¸ªåŒ…å«é¢„æµ‹ç»“æœçš„åˆ—è¡¨æˆ–æ•°ç»„ã€‚æ¯ä¸ªå…ƒç´ åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå› ä¸ºæ¥ä¸‹æ¥çš„æ“ä½œæ˜¯å¯¹å­—ç¬¦ä¸²è¿›è¡Œå¤„ç†ã€‚
            #strip()ç”¨äºå»é™¤å­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦ã€‚
            pred = decoded_preds[i].strip().lower()
            #print('pred0ï¼š', pred)
            #pred[-1]: è¿™è¡¨ç¤ºå–å­—ç¬¦ä¸²predçš„æœ€åä¸€ä¸ªå­—ç¬¦ã€‚æ¡ä»¶åˆ¤æ–­ï¼Œæ£€æŸ¥æœ€åä¸€ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯å¥å·ï¼ˆ"."ï¼‰
            #å¦‚æœ pred çš„æœ€åä¸€ä¸ªå­—ç¬¦æ˜¯å¥å·ï¼Œé‚£ä¹ˆå°† pred ä¸­é™¤äº†æœ€åä¸€ä¸ªå­—ç¬¦ä¹‹å¤–çš„æ‰€æœ‰å­—ç¬¦èµ‹ç»™ predã€‚
            #å¦‚æœ pred çš„æœ€åä¸€ä¸ªå­—ç¬¦ä¸æ˜¯å¥å·ï¼Œé‚£ä¹ˆ pred çš„å€¼ä¿æŒä¸å˜ã€‚
            pred = pred[:-1] if pred[-1] == "." else pred
            # if pred and pred[-1] == ".":
            #     pred = pred[:-1]
            #ä½¿ç”¨SPLIT = "; " ä½œä¸ºåˆ†éš”ç¬¦ï¼Œå°†å­—ç¬¦ä¸²åˆ†å‰²æˆä¸€ä¸ªåˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
            pred = pred.split(SPLIT.strip())
            #print('pred1ï¼š', pred)
            # remove the repeated words,æŠŠ pred ä¸­çš„å­—ç¬¦ä¸²æŒ‰ç…§é•¿åº¦ä»é•¿åˆ°çŸ­æ’åºã€‚
            pred = sorted(pred, key=lambda x: len(x), reverse=True)
            print('pred2ï¼š', pred)
            # label -> summary
            label = labels[i]

            # è¡¨ç¤ºä» label ä¸­ç§»é™¤æ‰€æœ‰å€¼ä¸º -100 çš„å…ƒç´ ã€‚åœ¨flan-t5-large -100 é€šå¸¸ç”¨äºè¡¨ç¤ºå¡«å……æˆ–æ— æ•ˆçš„æ ‡ç­¾ã€‚
            label = label[label != -100]
            # è¡¨ç¤ºä» label ä¸­ç§»é™¤æ‰€æœ‰å€¼ä¸º 1 çš„å…ƒç´ ã€‚åœ¨bart-large-cnn 1 é€šå¸¸ç”¨äºè¡¨ç¤ºå¡«å……æ ‡ç­¾ã€‚
            # label = label[label != -100]
            # label = label[label!=1]
            #å®ƒå°†å¤„ç†åçš„æ ‡ç­¾ label è½¬æ¢ä¸ºä¸€ä¸ªé”®ï¼Œå¯èƒ½æ˜¯ç”¨äºæŸ¥æ‰¾æŸä¸ªæ˜ å°„æˆ–å­—å…¸çš„å…³é”®å­—ã€‚
            label_key = convert_label_to_key(label)
            #å–å‡ºä¸ label_key å¯¹åº”çš„å€¼,ä¹Ÿå°±æ˜¯summary
            label_summary = val_test_summary_mapping[label_key].lower()
            print('label_summaryï¼š', label_summary)#str

            #label = val_test_summary_mapping[label_key].lower()


            # hit_pred åˆ—è¡¨å°†åŒ…å«é‚£äº›é¢„æµ‹ç»“æœä¸­å‘½ä¸­æ ‡ç­¾ label ä¸”ä¸åœ¨é¿å…å…³é”®è¯åˆ—è¡¨ä¸­çš„å…ƒç´ ã€‚
            # è¿™ä¸ªè¿‡ç¨‹å¯èƒ½æ˜¯ä¸ºäº†ç­›é€‰å‡ºæ¨¡å‹é¢„æµ‹çš„ç»“æœä¸­ä¸çœŸå®æ ‡ç­¾åŒ¹é…çš„å…³é”®è¯ï¼ŒåŒæ—¶æ’é™¤ä¸€äº›ä¸å¸Œæœ›å‡ºç°çš„å…³é”®è¯ã€‚
            hit_pred = []
            for p in pred:
                p = p.strip()
                # " ".join(hit_pred)æ˜¯å°†hit_predä¸­çš„æ‰€æœ‰å•è¯ç”¨ç©ºæ ¼è¿æ¥ï¼Œå½¢æˆçš„å­—ç¬¦ä¸²
                if p not in " ".join(hit_pred) and p in label_summary and p not in avoid_keywords:
                    hit_pred.append(p)

            # calculate hit score and precision
            # predï¼šé¢„æµ‹çš„å…³é”®è¯æ•°ï¼ˆå…³é”®è¯å¯èƒ½æ˜¯ä¸€ä¸ªå•è¯ã€ä¹Ÿå¯èƒ½æ˜¯ä¸€ç»„å•è¯ï¼‰ï¼›hit_numï¼šå‘½ä¸­çš„å…³é”®è¯æ•°ï¼›hit_precisionï¼šå‘½ä¸­ç²¾ç¡®ç‡
            n = len(pred)
            hit_num = len(hit_pred)
            hit_precision = hit_num / n if n > 0 else 0
            print('é¢„æµ‹å…³é”®è¯é•¿åº¦',n)
            print('å‘½ä¸­å…³é”®è¯é•¿åº¦', hit_num)
            print('å‘½ä¸­å…³é”®è¯ç²¾ç¡®ç‡hit_precision', hit_precision)

            # # labelï¼šlabelçš„å•è¯é•¿åº¦(å•è¯æ•°é‡)ï¼›hit_word_numï¼šå‘½ä¸­çš„å•è¯æ•°ï¼›hit_word_recallï¼šå‘½ä¸­å¬å›ç‡
            # m = len(label_summary.split())
            # hit_word_pred = " ".join(hit_pred).split()
            # hit_word_num = len(hit_word_pred)
            # hit_word_recall = hit_word_num / m if m > 0 else 0
            # print('label_summaryå•è¯é•¿åº¦', m)
            # print('å‘½ä¸­å•è¯é•¿åº¦', hit_word_num)
            # print('å‘½ä¸­ç‡å•è¯å¬å›ç‡hit_recall', hit_word_recall)

            hit_pred_word = []
            for p in " ".join(pred).split():
                p = p.strip()
                if p not in " ".join(hit_pred_word) and p in label_summary and p not in avoid_keywords:
                    hit_pred_word.append(p)

            m = len(label_summary.split())
            hit_word_num = len(hit_pred_word)
            hit_word_recall = hit_word_num / m if m > 0 else 0
            print('summaryå•è¯æ•°é‡', m)
            print('ç›®æ ‡æ‘˜è¦ä¸­â€”â€”å‘½ä¸­å•è¯æ•°é‡', hit_word_num)
            print('å‘½ä¸­ç‡å•è¯å¬å›ç‡hit_recall', hit_word_recall)


            # store results
            hint_precisions.append(hit_precision)
            hint_hit_key_nums.append(hit_num)
            hint_key_nums.append(n)

            hint_recalls.append(hit_word_recall)
            hint_hit_word_nums.append(hit_word_num)
            hint_word_nums.append(m)

        # Extract a few results
        #result = {"hint_hit_key_num": np.mean(hint_hit_key_nums), "hint_key_precision": np.mean(hint_precisions), "key_num": np.mean(hint_key_nums)}

        result = {"hint_hit_key_num": np.mean(hint_hit_key_nums), "hint_key_precision": np.mean(hint_precisions), "key_num": np.mean(hint_key_nums),
                  "hint_hit_word_num": np.mean(hint_hit_word_nums), "hint_word_recall": np.mean(hint_recalls), "word_num": np.mean(hint_word_nums)}
        
        # Add mean generated length
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
        result["gen_len"] = np.mean(prediction_lens)
        
        return {k: round(v, 4) for k, v in result.items()}
    
    # tokenize the dataset
    if task == 'summarization':
        compute_metrics = compute_rouge_metrics
        best_metric = "rouge1"

    elif task == 'extraction':
        compute_metrics = compute_hit_metrics
        best_metric = "loss"

    # arguments
    training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir, # output directory
    # output_dir=output_dir if not push_to_hub else hf_path, # output directory
    num_train_epochs=epochs, # total number of training epochs
    per_device_train_batch_size=train_batch_size,  # batch size per device during training
    per_device_eval_batch_size=eval_batch_size,   # batch size for evaluation
    evaluation_strategy='steps',
    # length_penalty = length_penalty,
    # no_repeat_ngram_size = no_repeat_ngram_size,
    learning_rate=learning_rate, # 2e-5
    weight_decay=0.01,
    # fp16=True,
    # è®¾ç½®è¿™ä¸‰ä¸ªå‚æ•°ä¸ºç›¸åŒçš„å€¼å¯ä»¥ç®€åŒ–è®­ç»ƒé…ç½®ï¼Œä½¿å¾—æ—¥å¿—è®°å½•ã€è¯„ä¼°å’Œæ¨¡å‹ä¿å­˜éƒ½åœ¨ç›¸åŒçš„é¢‘ç‡ä¸‹è¿›è¡Œï¼Œæ›´å®¹æ˜“ç®¡ç†å’Œç†è§£æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
    # å¤šå°‘ä¸ªè®­ç»ƒæ­¥ä¹‹åè®°å½•æ—¥å¿—/è¯„ä¼°æ¨¡å‹/æƒé‡æ¨¡å‹
    logging_steps=logging_steps, # the same as eval_stepï¼Œå¦‚æœ logging_steps è¢«è®¾ç½®ä¸º 100ï¼Œé‚£ä¹ˆåœ¨æ¯è®­ç»ƒ 100 ä¸ªæ­¥éª¤åï¼Œæ¨¡å‹å°±ä¼šè¾“å‡ºä¸€æ¬¡è®­ç»ƒæ—¥å¿—ã€‚
    eval_steps=logging_steps, # åœ¨å¤šå°‘ä¸ªè®­ç»ƒæ­¥ä¹‹åè¿›è¡Œä¸€æ¬¡æ¨¡å‹è¯„ä¼°
    save_steps=logging_steps, # doesn't work if load_best_model_at_end=True, will save every eval_steps (logging_steps)
    logging_dir=os.path.join(output_dir, "runs/"),
    save_total_limit=save_total_limit,# save_total_limit=3ï¼Œæ¯å½“ç”Ÿæˆä¸€ä¸ªæ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ—¶ï¼Œç³»ç»Ÿä¼šä¿ç•™æœ€æ–°çš„ 3 ä¸ªæ£€æŸ¥ç‚¹ï¼Œè€Œåˆ é™¤æ›´æ—§çš„æ£€æŸ¥ç‚¹ã€‚
    seed=seed,
    push_to_hub=push_to_hub,
    predict_with_generate=True, # for evaluation metrics
    # load_best_model_at_end=True,
    # metric_for_best_model=best_metric,
    remove_unused_columns=True
    )

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)

    trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    # callbacks = [EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]    
    )
    
    if do_train:
        train_results = trainer.train()
        print('train_results',train_results)
        eval_results = trainer.evaluate()
        print('train_results',eval_results)
        # save model locally and push it to the hub
        trainer.save_model(os.path.join(output_dir, "best_checkpoint/"))
        print(f'Save best model in {os.path.join(output_dir, "best_checkpoint/")}')
        if push_to_hub:
            trainer.push_to_hub()

    # inference on the test set
    if do_inference:
        #test_results = trainer.predict(test_dataset)
        test_results = trainer.predict(test_dataset, min_length=5)
        print('test_results',test_results)
    
    return model


def main():

    parser = argparse.ArgumentParser()
    # arguments for dataset
    parser.add_argument('--dataset', type=str, default='cnndm', choices=['cnndm','general','attack']) #

    parser.add_argument('--n_train', type=int, default=2000) #
    parser.add_argument('--n_val', type=int, default=500) #
    parser.add_argument('--n_test', type=int, default=500) #
    parser.add_argument('--extraction_mode', type=str, default='textrank', choices=['textrank', 'patternrank', 'keybert', 'yake', 'prompt'])
    parser.add_argument('--extraction_source', type=str, default='all', choices=['all', 'article', 'summary'])
    #max_ctx_lenè¾“å…¥é•¿åº¦ï¼Œmax_tgt_lenè¾“å‡ºé•¿åº¦ï¼Œå¯è®¾ç½®
    parser.add_argument('--max_ctx_len', type=int, default=512)
    parser.add_argument('--max_tgt_len', type=int, default=64)

    # arguments for huggingface training
    parser.add_argument('--model', type=str, default='t5-base')
    # è¾“å‡ºç›®å½•åœ¨ä»£ç ä¸­å®šä¹‰
    parser.add_argument('--output_dir', type=str, default=None)
    parser.add_argument('--load_strategy', type=str, default='load_initial', choices=['load_initial', 'load_best', 'load_last']) #

    parser.add_argument('--seed', type=int, default=1799)
    parser.add_argument('--save_total_limit', type=int, default=5)
    parser.add_argument('--epochs', type=int, default=2)
    parser.add_argument('--train_batch_size', type=int, default=4)
    parser.add_argument('--eval_batch_size', type=int, default=4)
    parser.add_argument('--logging_steps', type=int, default=100)
    parser.add_argument('--save_steps', type=int, default=2000)
    parser.add_argument('--learning_rate', type=float, default=2e-5)
    parser.add_argument('--early_stopping_patience', type=int, default=5)
    parser.add_argument('--length_penalty', type=float, default=2)
    parser.add_argument('--no_repeat_ngram_size', type=float, default=3)

    # training task
    parser.add_argument('--task', type=str, default='extraction', choices=['extraction', 'summarization'])

    parser.add_argument('--do_train', action='store_true')
    parser.add_argument('--do_inference', action='store_true')
    parser.add_argument('--push_to_hub', action='store_true')
    
    args, unknown = parser.parse_known_args()

    dataset = args.dataset
    task = args.task
    load_strategy = args.load_strategy
    n_train, n_val, n_test = args.n_train, args.n_val, args.n_test
    extraction_mode = args.extraction_mode
    extraction_source = args.extraction_source
    max_ctx_len = args.max_ctx_len
    max_tgt_len = args.max_tgt_len
    length_penalty = args.length_penalty
    no_repeat_ngram_size = args.no_repeat_ngram_size
    assert max_ctx_len <= 1024 # for T5
    # åœ¨æºä»£ç ä»£ç ä¸­args.modelæ˜¯æ¨¡å‹çš„åå­—ï¼Œmodel_pathæ˜¯æ¨¡å‹çš„ä¸‹è½½è·¯å¾„æœ‰å…³
    # åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œmodel_nameæ˜¯æ¨¡å‹çš„åå­—ï¼Œmodel_pathæ˜¯æ¨¡å‹çš„è·¯å¾„
    model_path = args.model
    model_name = model_path.split('/')[-1]

    # if args.model in ['t5-base', 't5-large', 't5-3b']:
    #     model_path = args.model
    # elif args.model in ['flan-t5-large', 'flan-t5-base', 'flan-t5-small']:
    #     model_path = f"google/{args.model}"

    """prepare for training"""
    if args.output_dir is None:
        if task == 'summarization':
            output_dir = f"./sft4lms/ckpt/{dataset}_{n_train}_{max_ctx_len}_{max_tgt_len}/summarization/{model_name}/"
        elif task == 'extraction':
            output_dir = f"./sft4lms/ckpt/{dataset}_{n_train}_{max_ctx_len}_{max_tgt_len}/{extraction_mode}-{extraction_source}/{model_name}-ep{args.epochs}"
    else:
        output_dir = args.output_dir

    # LOAD cnndm DATAï¼š/bin/zsh ./run_sft_cnndm.sh
    # Ptrain, Pval, Ptest = get_data_split(dataset, n_train, n_val, n_test, extraction_mode, extraction_source)
    # LOAD general lDATAï¼š/bin/zsh ./run_sft_general.sh
    Ptrain, Pval, Ptest = get_general_data_split(dataset, n_train, n_val, n_test, extraction_mode, extraction_source)
    # LOAD attack lDATAï¼š/bin/zsh ./run_sft_attack.sh
    # Ptrain, Pval, Ptest = get_attack_data_split(dataset, n_train, n_val, n_test, extraction_mode, extraction_source)

    # LOAD MODEL
    model, tokenizer = load_model(output_dir, model_path, load_strategy)

    fine_tune_hf(
    task=args.task,
    # model_name=args.model,
    model_name=model_name,
    dataset_name=args.dataset,
    n_train=n_train,
    push_to_hub=args.push_to_hub,
    model=model,
    tokenizer=tokenizer,
    extraction_source=extraction_source,
    max_ctx_len=max_ctx_len,
    max_tgt_len=max_tgt_len,
    length_penalty=length_penalty,
    no_repeat_ngram_size=no_repeat_ngram_size,
    output_dir=output_dir,
    train_data=Ptrain,
    val_data=Pval,
    test_data=Ptest,
    epochs=args.epochs,
    train_batch_size=args.train_batch_size,
    eval_batch_size=args.eval_batch_size,
    logging_steps=args.logging_steps,
    save_total_limit=args.save_total_limit,
    early_stopping_patience=args.early_stopping_patience,
    learning_rate=args.learning_rate,
    seed=args.seed,
    do_train=args.do_train,
    do_inference=args.do_inference
    )


if __name__ == "__main__":
    main()

